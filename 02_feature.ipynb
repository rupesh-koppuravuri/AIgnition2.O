{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c59b631a",
   "metadata": {},
   "source": [
    "1: Metadata & Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256a1a86",
   "metadata": {},
   "source": [
    "# AIgnition 2.0 - Feature Engineering Pipeline\n",
    "## Notebook 02: Production-Scale Feature Generation for Personalization\n",
    "\n",
    "**Mission**: Transform clean session data into powerful personalization features\n",
    "**Input**: 1M+ sessions with transaction attribution from Notebook 01\n",
    "**Output**: Feature-rich dataset ready for ML recommendation engine\n",
    "**Performance**: Memory-optimized for 16GB RAM with <4GB peak usage\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Feature Engineering Objectives\n",
    "\n",
    "### **User Behavior Features**\n",
    "- âœ… **RFM Analysis**: Recency, Frequency, Monetary segmentation\n",
    "- âœ… **Session Patterns**: Duration, event density, conversion tracking\n",
    "- âœ… **Journey Mapping**: Page path sequences and user flow analysis\n",
    "- âœ… **Device Intelligence**: Cross-device behavior and preferences\n",
    "\n",
    "### **Business Intelligence Features**  \n",
    "- âœ… **Geographic Insights**: Regional purchasing patterns and preferences\n",
    "- âœ… **Temporal Analysis**: Time-based behavior and seasonality patterns\n",
    "- âœ… **Revenue Attribution**: Purchase behavior and customer lifetime value\n",
    "- âœ… **Product Affinity**: Category preferences and cross-selling opportunities\n",
    "\n",
    "### **Cold Start Strategy Features**\n",
    "- âœ… **Anonymous User Signals**: Device, location, traffic source intelligence\n",
    "- âœ… **Behavioral Proxies**: Similar user pattern matching\n",
    "- âœ… **Demographic Inference**: Age, income, and preference estimation\n",
    "- âœ… **Real-time Adaptability**: Fast feature computation for new users\n",
    "\n",
    "**ğŸ† OUTCOME**: Rich feature matrix enabling hyper-personalized landing pages for both known and anonymous users\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c98e83",
   "metadata": {},
   "source": [
    "2: Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d852b6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Environment ready for feature engineering\n",
      "ğŸ“Š Target: Transform 1M+ sessions into personalization features\n",
      "ğŸ’¾ Current memory usage: 11.5GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11.532142639160156"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Core libraries optimized for memory efficiency\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Progress tracking and memory monitoring\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "import gc\n",
    "\n",
    "# Feature engineering libraries\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Configuration paths (using outputs from Notebook 01)\n",
    "PROCESSED_DIR = Path(\"../data/processed\")\n",
    "PARQUET_DIR = Path(\"../data/parquet\") \n",
    "FEATURES_DIR = Path(\"../data/features\")\n",
    "FEATURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Memory monitoring function\n",
    "def check_memory():\n",
    "    memory_gb = psutil.virtual_memory().used / (1024**3)\n",
    "    print(f\"ğŸ’¾ Current memory usage: {memory_gb:.1f}GB\")\n",
    "    return memory_gb\n",
    "\n",
    "print(\"âœ… Environment ready for feature engineering\")\n",
    "print(\"ğŸ“Š Target: Transform 1M+ sessions into personalization features\")\n",
    "check_memory()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612deed2",
   "metadata": {},
   "source": [
    "3: Load Processed Data from Notebook 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c427bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Loading processed data from Notebook 01...\n",
      "âœ… Loaded events: 6,602,856 records\n",
      "âœ… Loaded sessions: 1,004,683 sessions\n",
      "âœ… Loaded transactions: 27,500 transaction records\n",
      "\n",
      "ğŸ“Š Data Overview:\n",
      "  â€¢ Unique users: 744,675\n",
      "  â€¢ Unique sessions: 1,004,683\n",
      "  â€¢ Date range: 2024-06-11 00:00:00 to 2025-06-07 00:00:00\n",
      "  â€¢ Revenue coverage: $3,659,103.12\n",
      "ğŸ’¾ Current memory usage: 14.3GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14.278186798095703"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load clean, processed data from Notebook 01\n",
    "print(\"ğŸ“‚ Loading processed data from Notebook 01...\")\n",
    "\n",
    "# Load main merged dataset\n",
    "events_merged = pd.read_parquet(PROCESSED_DIR / \"events_merged_final.parquet\")\n",
    "print(f\"âœ… Loaded events: {len(events_merged):,} records\")\n",
    "\n",
    "# Load session summary for efficient processing\n",
    "session_summary = pd.read_parquet(PROCESSED_DIR / \"session_summary.parquet\")\n",
    "print(f\"âœ… Loaded sessions: {len(session_summary):,} sessions\")\n",
    "\n",
    "# Load transactions for product analysis\n",
    "transactions = pd.read_parquet(PROCESSED_DIR / \"transactions_optimized.parquet\")\n",
    "print(f\"âœ… Loaded transactions: {len(transactions):,} transaction records\")\n",
    "\n",
    "# Data overview\n",
    "print(f\"\\nğŸ“Š Data Overview:\")\n",
    "print(f\"  â€¢ Unique users: {events_merged['user_pseudo_id'].nunique():,}\")\n",
    "print(f\"  â€¢ Unique sessions: {events_merged['session_id'].nunique():,}\")\n",
    "print(f\"  â€¢ Date range: {events_merged['eventDate'].min()} to {events_merged['eventDate'].max()}\")\n",
    "print(f\"  â€¢ Revenue coverage: ${events_merged['Item_revenue'].sum():,.2f}\")\n",
    "\n",
    "check_memory()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ff8098",
   "metadata": {},
   "source": [
    " 4: User-Level RFM Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "066e96d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‘¥ Generating User-Level RFM Features...\n",
      "âœ… Generated RFM features for 744,675 users\n",
      "ğŸ“Š Average metrics:\n",
      "  â€¢ Sessions per user: 1.3\n",
      "  â€¢ Revenue per user: $4.91\n",
      "  â€¢ Conversion rate: 0.021\n"
     ]
    }
   ],
   "source": [
    "# Create comprehensive RFM features for user segmentation\n",
    "print(\"ğŸ‘¥ Generating User-Level RFM Features...\")\n",
    "\n",
    "# Calculate reference date for recency\n",
    "ref_date = events_merged['eventTimestamp'].max()\n",
    "\n",
    "# User-level aggregations with memory optimization\n",
    "user_rfm = events_merged.groupby('user_pseudo_id').agg({\n",
    "    'eventTimestamp': ['min', 'max', 'count'],\n",
    "    'session_id': 'nunique',\n",
    "    'Item_revenue': ['sum', 'count'],\n",
    "    'event_name': lambda x: (x == 'purchase').sum(),\n",
    "    'eventDate': lambda x: x.nunique()\n",
    "}).round(2)\n",
    "\n",
    "# Flatten column names\n",
    "user_rfm.columns = ['first_seen', 'last_seen', 'total_events', 'total_sessions', \n",
    "                   'total_revenue', 'revenue_events', 'purchase_events', 'active_days']\n",
    "\n",
    "# Calculate RFM metrics\n",
    "user_rfm['recency_days'] = (ref_date - user_rfm['last_seen']).dt.days\n",
    "user_rfm['frequency_score'] = user_rfm['total_sessions']\n",
    "user_rfm['monetary_score'] = user_rfm['total_revenue']\n",
    "user_rfm['avg_session_value'] = user_rfm['total_revenue'] / user_rfm['total_sessions']\n",
    "user_rfm['conversion_rate'] = user_rfm['purchase_events'] / user_rfm['total_sessions']\n",
    "\n",
    "# Handle missing values\n",
    "user_rfm = user_rfm.fillna(0)\n",
    "\n",
    "print(f\"âœ… Generated RFM features for {len(user_rfm):,} users\")\n",
    "print(f\"ğŸ“Š Average metrics:\")\n",
    "print(f\"  â€¢ Sessions per user: {user_rfm['total_sessions'].mean():.1f}\")\n",
    "print(f\"  â€¢ Revenue per user: ${user_rfm['total_revenue'].mean():.2f}\")\n",
    "print(f\"  â€¢ Conversion rate: {user_rfm['conversion_rate'].mean():.3f}\")\n",
    "\n",
    "# Cleanup\n",
    "#del user_rfm['first_seen'], user_rfm['last_seen']\n",
    "#gc.collect()\n",
    "#check_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e046b31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Current memory usage: 12.0GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12.012710571289062"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9496cf85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Current memory usage: 12.9GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12.878948211669922"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# âœ… UNCOMMENT THESE LINES IMMEDIATELY\n",
    "del user_rfm['first_seen'], user_rfm['last_seen']\n",
    "gc.collect()\n",
    "check_memory()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0796bc37",
   "metadata": {},
   "source": [
    "5: Geographic & Demographic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f45f8669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŒ Generating Geographic & Demographic Features (Chunked)...\n",
      "âœ… Generated geographic features for 744,675 users\n",
      "ğŸ“Š Top regions: {'New York': 122564, 'California': 103690, 'Unknown': 75329, 'Texas': 45791, 'Florida': 41180}\n",
      "ğŸ“Š Device types: {'desktop': 394263, 'mobile': 338012, 'tablet': 12354, 'smart tv': 46}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"ğŸŒ Generating Geographic & Demographic Features (Chunked)...\")\n",
    "\n",
    "chunk_size = 100_000\n",
    "user_ids = events_merged['user_pseudo_id'].unique()\n",
    "geo_features_list = []\n",
    "\n",
    "for i in range(0, len(user_ids), chunk_size):\n",
    "    chunk_users = user_ids[i:i+chunk_size]\n",
    "    chunk = events_merged[events_merged['user_pseudo_id'].isin(chunk_users)]\n",
    "    features = chunk.groupby('user_pseudo_id').agg({\n",
    "        'city': lambda x: x.mode().iloc[0] if not x.mode().empty else 'Unknown',\n",
    "        'region': lambda x: x.mode().iloc[0] if not x.mode().empty else 'Unknown',\n",
    "        'country': lambda x: x.mode().iloc[0] if not x.mode().empty else 'Unknown',\n",
    "        'category': lambda x: x.mode().iloc[0] if not x.mode().empty else 'Unknown',\n",
    "        'source': lambda x: x.mode().iloc[0] if not x.mode().empty else 'Unknown',\n",
    "        'gender': lambda x: x.mode().iloc[0] if not x.mode().empty else 'Unknown',\n",
    "        'Age': lambda x: x.mode().iloc[0] if not x.mode().empty else 'Unknown',\n",
    "        'income_group': lambda x: x.mode().iloc[0] if not x.mode().empty else 'Unknown'\n",
    "    })\n",
    "    geo_features_list.append(features)\n",
    "    del chunk, features\n",
    "\n",
    "geo_features = pd.concat(geo_features_list)\n",
    "geo_features.columns = [\n",
    "    'primary_city', 'primary_region', 'primary_country',\n",
    "    'dominant_device', 'primary_source', 'primary_gender',\n",
    "    'primary_age', 'primary_income'\n",
    "]\n",
    "del geo_features_list\n",
    "\n",
    "# Device & source diversity (also chunked)\n",
    "device_diversity_list = []\n",
    "source_diversity_list = []\n",
    "\n",
    "for i in range(0, len(user_ids), chunk_size):\n",
    "    chunk_users = user_ids[i:i+chunk_size]\n",
    "    chunk = events_merged[events_merged['user_pseudo_id'].isin(chunk_users)]\n",
    "    device_div = chunk.groupby('user_pseudo_id')['category'].nunique()\n",
    "    source_div = chunk.groupby('user_pseudo_id')['source'].nunique()\n",
    "    device_diversity_list.append(device_div)\n",
    "    source_diversity_list.append(source_div)\n",
    "    del chunk, device_div, source_div\n",
    "\n",
    "device_diversity = pd.concat(device_diversity_list).rename('device_diversity')\n",
    "source_diversity = pd.concat(source_diversity_list).rename('source_diversity')\n",
    "\n",
    "geo_features = geo_features.join([device_diversity, source_diversity])\n",
    "\n",
    "print(f\"âœ… Generated geographic features for {len(geo_features):,} users\")\n",
    "print(f\"ğŸ“Š Top regions: {geo_features['primary_region'].value_counts().head(5).to_dict()}\")\n",
    "print(f\"ğŸ“Š Device types: {geo_features['dominant_device'].value_counts().to_dict()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854859a3",
   "metadata": {},
   "source": [
    "6: Session-Level Behavioral Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8513becd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Generating Session-Level Behavioral Features (Chunked)...\n",
      "âœ… Generated session features for 1,004,683 sessions\n",
      "ğŸ“Š Session patterns:\n",
      "ğŸ“Š Avg duration: 2.1 min\n",
      "  â€¢ Avg events per minute: 2.92\n",
      "ğŸ“Š Purchase sessions: 18,076\n"
     ]
    }
   ],
   "source": [
    "# Generate session-level behavioral patterns\n",
    "print(\"ğŸ”„ Generating Session-Level Behavioral Features (Chunked)...\")\n",
    "\n",
    "chunk_size = 200_000\n",
    "session_ids = session_summary['session_id'].unique() # Load session data efficiently\n",
    "session_features_list = []\n",
    "\n",
    "for i in range(0, len(session_ids), chunk_size):\n",
    "    chunk_sessions = session_ids[i:i+chunk_size]\n",
    "    chunk = session_summary[session_summary['session_id'].isin(chunk_sessions)].copy()\n",
    "\n",
    "    \n",
    "    # Add session timing features\n",
    "    chunk['session_hour'] = pd.to_datetime(chunk['session_start']).dt.hour\n",
    "    chunk['session_dow'] = pd.to_datetime(chunk['session_start']).dt.dayofweek\n",
    "    chunk['is_weekend'] = chunk['session_dow'].isin([5, 6])\n",
    "    chunk['duration_minutes'] = (\n",
    "        pd.to_datetime(chunk['session_end']) - pd.to_datetime(chunk['session_start'])\n",
    "    ).dt.total_seconds() / 60\n",
    "\n",
    "    # Session engagement metrics\n",
    "    chunk['events_per_minute'] = chunk['event_count'] / (chunk['duration_minutes'] + 1)\n",
    "    chunk['has_purchase'] = chunk['revenue'] > 0\n",
    "    chunk['session_value'] = chunk['revenue'].fillna(0)\n",
    "    chunk['time_of_day'] = pd.cut(chunk['session_hour'], bins=[0, 6, 12, 18, 24],\n",
    "                                  labels=['night', 'morning', 'afternoon', 'evening']) # Time-based features\n",
    "    session_features_list.append(chunk)\n",
    "    del chunk\n",
    "\n",
    "session_features = pd.concat(session_features_list)\n",
    "del session_features_list\n",
    "\n",
    "print(f\"âœ… Generated session features for {len(session_features):,} sessions\")\n",
    "print(f\"ğŸ“Š Session patterns:\")\n",
    "print(f\"ğŸ“Š Avg duration: {session_features['duration_minutes'].mean():.1f} min\")\n",
    "print(f\"  â€¢ Avg events per minute: {session_features['events_per_minute'].mean():.2f}\")\n",
    "print(f\"ğŸ“Š Purchase sessions: {session_features['has_purchase'].sum():,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f19b48d",
   "metadata": {},
   "source": [
    "7: User Journey & Page Path Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94aee5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ—ºï¸ Generating User Journey Features (Chunked)...\n",
      "âœ… Generated journey features for 744,675 users\n",
      "ğŸ“Š Avg page diversity: 5.4\n"
     ]
    }
   ],
   "source": [
    "# Analyze user journey patterns for personalization\n",
    "print(\"ğŸ—ºï¸ Generating User Journey Features (Chunked)...\")\n",
    "\n",
    "chunk_size = 100_000\n",
    "user_ids = events_merged['user_pseudo_id'].unique()\n",
    "page_pref_list = []\n",
    "journey_features_list = []\n",
    "\n",
    "for i in range(0, len(user_ids), chunk_size):\n",
    "    chunk_users = user_ids[i:i+chunk_size]\n",
    "    chunk = events_merged[events_merged['user_pseudo_id'].isin(chunk_users)].copy()\n",
    "    page_paths = chunk[chunk['page_path'].notna()].copy()  # Page path analysis (memory efficient approach)\n",
    "\n",
    "    # Extract page types from paths\n",
    "    page_paths['page_category'] = page_paths['page_path'].str.extract(r'/(\\w+)/')\n",
    "    page_paths['page_category'] = page_paths['page_category'].fillna('homepage')\n",
    "\n",
    "    # User-level page preferences\n",
    "    page_pref = page_paths.groupby('user_pseudo_id').agg({\n",
    "        'page_category': lambda x: x.mode().iloc[0] if not x.mode().empty else 'homepage',\n",
    "        'page_path': 'nunique'\n",
    "    }).rename(columns={'page_category': 'preferred_page_type', 'page_path': 'page_diversity'})\n",
    "    page_pref_list.append(page_pref)\n",
    "\n",
    "    # Journey depth analysis\n",
    "    journey = chunk.groupby(['user_pseudo_id', 'session_id']).agg({\n",
    "        'event_name': lambda x: ' â†’ '.join(x.unique()[:5]),\n",
    "        'page_type': lambda x: x.mode().iloc[0] if not x.mode().empty else 'unknown'\n",
    "    }).reset_index()\n",
    "    journey_features_list.append(journey)\n",
    "    del chunk, page_paths, page_pref, journey\n",
    "\n",
    "page_preferences = pd.concat(page_pref_list)\n",
    "journey_features = pd.concat(journey_features_list)\n",
    "\n",
    "# User journey patterns\n",
    "user_journeys = journey_features.groupby('user_pseudo_id').agg({\n",
    "    'event_name': lambda x: len(set(' â†’ '.join(x).split(' â†’ '))),\n",
    "    'page_type': lambda x: x.mode().iloc[0] if not x.mode().empty else 'unknown'\n",
    "}).rename(columns={'event_name': 'journey_complexity', 'page_type': 'preferred_entry_page'})\n",
    "\n",
    "# Merge journey features\n",
    "journey_final = page_preferences.join(user_journeys, how='outer').fillna('unknown')\n",
    "del page_pref_list, journey_features_list, page_preferences, journey_features, user_journeys\n",
    "\n",
    "print(f\"âœ… Generated journey features for {len(journey_final):,} users\")\n",
    "print(f\"ğŸ“Š Journey patterns:\")\n",
    "print(f\"  â€¢ Avg page diversity: {journey_final['page_diversity'].mean():.1f}\")\n",
    "print(f\"  â€¢ Top entry pages: {journey_final['preferred_entry_page'].value_counts().head(3).to_dict()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd77250",
   "metadata": {},
   "source": [
    "8: Product & Category Affinity Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9957cb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ›ï¸ Generating Product Affinity Features (Chunked)...\n",
      "âœ… Generated product features for 17,131 purchasing users\n",
      "ğŸ“Š Product preferences:\n",
      "  â€¢ Top categories: {'CATEGORY_1': 11143, 'CATEGORY_2': 5507, 'CATEGORY_3': 344}\n",
      "  â€¢ Price segments: {'premium': 5004, 'mid': 4941, 'budget': 4497, 'luxury': 2679}\n"
     ]
    }
   ],
   "source": [
    "# Generate product affinity features for recommendation engine\n",
    "print(\"ğŸ›ï¸ Generating Product Affinity Features (Chunked)...\")\n",
    "\n",
    "chunk_size = 100_000\n",
    "user_ids = events_merged['user_pseudo_id'].unique()\n",
    "product_features_list = []\n",
    "\n",
    "for i in range(0, len(user_ids), chunk_size):\n",
    "    chunk_users = user_ids[i:i+chunk_size]\n",
    "\n",
    "    # Product interaction analysis (only for users with transactions)\n",
    "    chunk = events_merged[(events_merged['user_pseudo_id'].isin(chunk_users)) & (events_merged['ItemName'].notna())].copy()\n",
    "    if len(chunk) == 0:\n",
    "        continue\n",
    "    # User-product preferences\n",
    "    features = chunk.groupby('user_pseudo_id').agg({\n",
    "        'ItemCategory': lambda x: x.mode().iloc[0] if not x.mode().empty else 'unknown',\n",
    "        'ItemBrand': lambda x: x.mode().iloc[0] if not x.mode().empty else 'unknown',\n",
    "        'ItemName': 'nunique',\n",
    "        'Item_revenue': ['mean', 'std'],\n",
    "        'Item_purchase_quantity': 'sum'\n",
    "    })\n",
    "\n",
    "    # Flatten columns\n",
    "    features.columns = ['preferred_category', 'preferred_brand', 'product_diversity',\n",
    "                        'avg_item_price', 'price_variance', 'total_quantity']\n",
    "    # Price sensitivity analysis\n",
    "    features['price_sensitivity'] = pd.cut(features['avg_item_price'],\n",
    "                                           bins=[0, 50, 100, 200, float('inf')],\n",
    "                                           labels=['budget', 'mid', 'premium', 'luxury'])\n",
    "    product_features_list.append(features)\n",
    "    del chunk, features\n",
    "\n",
    "if product_features_list:\n",
    "    product_features = pd.concat(product_features_list)\n",
    "else:\n",
    "    product_features = pd.DataFrame(index=user_ids[:0])\n",
    "    print(\"âš ï¸ No product transaction data found - creating empty product features\")\n",
    "\n",
    "#del product_features_list\n",
    "\n",
    "\n",
    "print(f\"âœ… Generated product features for {len(product_features):,} purchasing users\")\n",
    "print(f\"ğŸ“Š Product preferences:\")\n",
    "print(f\"  â€¢ Top categories: {product_features['preferred_category'].value_counts().head(3).to_dict()}\")\n",
    "print(f\"  â€¢ Price segments: {product_features['price_sensitivity'].value_counts().to_dict()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d934e623",
   "metadata": {},
   "source": [
    "9: Feature Integration & User Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cedd5fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DATA TYPES AFTER JOINS:\n",
      "==================================================\n",
      "OBJECT: primary_city - NaN count: 0\n",
      "OBJECT: primary_region - NaN count: 0\n",
      "OBJECT: primary_country - NaN count: 0\n",
      "OBJECT: dominant_device - NaN count: 0\n",
      "OBJECT: primary_source - NaN count: 0\n",
      "CATEGORICAL: primary_gender\n",
      "  Categories: Index(['female', 'male'], dtype='object')\n",
      "  Has 'unknown': False\n",
      "  NaN count: 0\n",
      "\n",
      "OBJECT: primary_age - NaN count: 0\n",
      "OBJECT: primary_income - NaN count: 0\n",
      "OBJECT: preferred_page_type - NaN count: 0\n",
      "OBJECT: preferred_entry_page - NaN count: 0\n",
      "CATEGORICAL: preferred_category\n",
      "  Categories: Index(['CATEGORY_1', 'CATEGORY_2', 'CATEGORY_3', 'CATEGORY_4', 'CATEGORY_5'], dtype='object')\n",
      "  Has 'unknown': False\n",
      "  NaN count: 727544\n",
      "\n",
      "CATEGORICAL: preferred_brand\n",
      "  Categories: Index(['ITEM_BRAND1', 'ITEM_BRAND2'], dtype='object')\n",
      "  Has 'unknown': False\n",
      "  NaN count: 727544\n",
      "\n",
      "CATEGORICAL: price_sensitivity\n",
      "  Categories: Index(['budget', 'mid', 'premium', 'luxury'], dtype='object')\n",
      "  Has 'unknown': False\n",
      "  NaN count: 727554\n",
      "\n",
      "ğŸ“Š Total categorical columns: 4\n"
     ]
    }
   ],
   "source": [
    "# Diagnostic: Check all data types after joins\n",
    "print(\"ğŸ” DATA TYPES AFTER JOINS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for col in master_features.columns:\n",
    "    dtype = master_features[col].dtype\n",
    "    if pd.api.types.is_categorical_dtype(dtype):\n",
    "        print(f\"CATEGORICAL: {col}\")\n",
    "        print(f\"  Categories: {master_features[col].cat.categories}\")\n",
    "        print(f\"  Has 'unknown': {'unknown' in master_features[col].cat.categories}\")\n",
    "        print(f\"  NaN count: {master_features[col].isna().sum()}\")\n",
    "        print()\n",
    "    elif dtype == 'object':\n",
    "        print(f\"OBJECT: {col} - NaN count: {master_features[col].isna().sum()}\")\n",
    "\n",
    "print(f\"ğŸ“Š Total categorical columns: {sum(pd.api.types.is_categorical_dtype(master_features[col]) for col in master_features.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2fc0578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”— Integrating Features & Creating User Segments...\n",
      "ğŸ”§ Converting all categorical columns to string type...\n",
      "  Converting primary_gender from categorical to string\n",
      "  Converting preferred_category from categorical to string\n",
      "  Converting preferred_brand from categorical to string\n",
      "  Converting price_sensitivity from categorical to string\n",
      "ğŸ”§ Filling NaN values in string columns...\n",
      "ğŸ”§ Filling NaN values in numerical columns...\n",
      "âœ… Remaining NaN values: 0\n",
      "ğŸ“Š Creating user segments...\n",
      "âœ… Integrated features for 744,675 users\n",
      "ğŸ“Š Feature matrix: (744675, 34)\n",
      "ğŸ¯ User segments: {'Champions': 381054, 'Potential_Loyalists': 363562, 'At_Risk': 53, 'New_Customers': 5, 'Loyal_Customers': 1}\n",
      "ğŸ’¾ Current memory usage: 13.0GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13.044563293457031"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 9: Feature Integration & User Segmentation (BULLETPROOF FIX)\n",
    "print(\"ğŸ”— Integrating Features & Creating User Segments...\")\n",
    "\n",
    "# Merge all user-level features\n",
    "master_features = user_rfm.copy()\n",
    "\n",
    "# Add geographic features\n",
    "master_features = master_features.join(geo_features, how='left')\n",
    "\n",
    "# Add journey features  \n",
    "master_features = master_features.join(journey_final, how='left')\n",
    "\n",
    "# Add product features (only for purchasing users)\n",
    "if len(product_features) > 0:\n",
    "    master_features = master_features.join(product_features, how='left')\n",
    "\n",
    "# ğŸ”§ BULLETPROOF CATEGORICAL HANDLING\n",
    "print(\"ğŸ”§ Converting all categorical columns to string type...\")\n",
    "\n",
    "# Convert ALL categorical columns to string to avoid category issues\n",
    "for col in master_features.columns:\n",
    "    if pd.api.types.is_categorical_dtype(master_features[col]):\n",
    "        print(f\"  Converting {col} from categorical to string\")\n",
    "        master_features[col] = master_features[col].astype(str)\n",
    "\n",
    "# Now handle string columns normally\n",
    "categorical_cols = ['primary_city', 'primary_region', 'dominant_device', 'primary_source',\n",
    "                   'primary_gender', 'primary_age', 'primary_income', 'preferred_page_type',\n",
    "                   'preferred_entry_page', 'preferred_category', 'preferred_brand', 'price_sensitivity']\n",
    "\n",
    "print(\"ğŸ”§ Filling NaN values in string columns...\")\n",
    "for col in categorical_cols:\n",
    "    if col in master_features.columns:\n",
    "        # Replace 'nan' string (from categorical conversion) with 'unknown'\n",
    "        master_features[col] = master_features[col].replace('nan', 'unknown')\n",
    "        master_features[col] = master_features[col].fillna('unknown')\n",
    "\n",
    "# Fill numerical columns with 0\n",
    "print(\"ğŸ”§ Filling NaN values in numerical columns...\")\n",
    "numerical_columns = master_features.select_dtypes(include=[np.number]).columns\n",
    "master_features[numerical_columns] = master_features[numerical_columns].fillna(0)\n",
    "\n",
    "# Verify no NaN values remain\n",
    "remaining_nans = master_features.isnull().sum().sum()\n",
    "print(f\"âœ… Remaining NaN values: {remaining_nans}\")\n",
    "\n",
    "if remaining_nans > 0:\n",
    "    print(\"âš ï¸ Some NaN values remain - filling all remaining with appropriate defaults\")\n",
    "    # Emergency fallback - fill any remaining NaNs\n",
    "    for col in master_features.columns:\n",
    "        if master_features[col].isnull().sum() > 0:\n",
    "            if master_features[col].dtype in ['object', 'string']:\n",
    "                master_features[col] = master_features[col].fillna('unknown')\n",
    "            else:\n",
    "                master_features[col] = master_features[col].fillna(0)\n",
    "\n",
    "# Create user segments using RFM clustering\n",
    "print(\"ğŸ“Š Creating user segments...\")\n",
    "\n",
    "# Prepare data for clustering (select key RFM features)\n",
    "cluster_features = master_features[['recency_days', 'frequency_score', 'monetary_score']].copy()\n",
    "cluster_features = cluster_features.fillna(0)\n",
    "\n",
    "# Standardize features for clustering\n",
    "scaler = StandardScaler()\n",
    "cluster_scaled = scaler.fit_transform(cluster_features)\n",
    "\n",
    "# K-means clustering for user segmentation\n",
    "kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)\n",
    "master_features['user_segment'] = kmeans.fit_predict(cluster_scaled)\n",
    "\n",
    "# Segment labels for business interpretation\n",
    "segment_labels = {0: 'Champions', 1: 'Loyal_Customers', 2: 'Potential_Loyalists', \n",
    "                 3: 'New_Customers', 4: 'At_Risk'}\n",
    "master_features['segment_label'] = master_features['user_segment'].map(segment_labels)\n",
    "\n",
    "print(f\"âœ… Integrated features for {len(master_features):,} users\")\n",
    "print(f\"ğŸ“Š Feature matrix: {master_features.shape}\")\n",
    "print(f\"ğŸ¯ User segments: {master_features['segment_label'].value_counts().to_dict()}\")\n",
    "\n",
    "check_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "660e6888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Data Types Analysis:\n",
      "total_events               int64\n",
      "total_sessions             int64\n",
      "total_revenue            float64\n",
      "revenue_events             int64\n",
      "purchase_events            int64\n",
      "active_days                int64\n",
      "recency_days               int64\n",
      "frequency_score            int64\n",
      "monetary_score           float64\n",
      "avg_session_value        float64\n",
      "conversion_rate          float64\n",
      "primary_city              object\n",
      "primary_region            object\n",
      "primary_country           object\n",
      "dominant_device           object\n",
      "primary_source            object\n",
      "primary_gender          category\n",
      "primary_age               object\n",
      "primary_income            object\n",
      "device_diversity           int64\n",
      "source_diversity           int64\n",
      "preferred_page_type       object\n",
      "page_diversity             int64\n",
      "journey_complexity         int64\n",
      "preferred_entry_page      object\n",
      "preferred_category      category\n",
      "preferred_brand         category\n",
      "product_diversity        float64\n",
      "avg_item_price           float64\n",
      "price_variance           float64\n",
      "total_quantity           float64\n",
      "price_sensitivity       category\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Check data types of all columns\n",
    "print(\"ğŸ” Data Types Analysis:\")\n",
    "print(master_features.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06c03519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Categorical Columns Analysis:\n",
      "primary_gender: Index(['female', 'male', 'unknown'], dtype='object')\n",
      "preferred_category: Index(['CATEGORY_1', 'CATEGORY_2', 'CATEGORY_3', 'CATEGORY_4', 'CATEGORY_5',\n",
      "       'unknown'],\n",
      "      dtype='object')\n",
      "preferred_brand: Index(['ITEM_BRAND1', 'ITEM_BRAND2', 'unknown'], dtype='object')\n",
      "price_sensitivity: Index(['budget', 'mid', 'premium', 'luxury', 'unknown'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Check which columns are categorical\n",
    "print(\"ğŸ” Categorical Columns Analysis:\")\n",
    "categorical_columns = []\n",
    "for col in master_features.columns:\n",
    "    if pd.api.types.is_categorical_dtype(master_features[col]):\n",
    "        categorical_columns.append(col)\n",
    "        print(f\"{col}: {master_features[col].cat.categories}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "abdb8a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Categorical Columns with NaNs:\n"
     ]
    }
   ],
   "source": [
    "# Check categorical columns that still have NaNs\n",
    "print(\"ğŸ” Categorical Columns with NaNs:\")\n",
    "for col in master_features.columns:\n",
    "    if pd.api.types.is_categorical_dtype(master_features[col]):\n",
    "        nan_count = master_features[col].isnull().sum()\n",
    "        if nan_count > 0:\n",
    "            print(f\"{col}: {nan_count} NaNs, Categories: {master_features[col].cat.categories}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3e61a6",
   "metadata": {},
   "source": [
    "10: Cold Start Feature Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24eb0603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ†• Generating Cold Start Feature Matrix (Chunked)...\n",
      "âœ… Generated cold start matrix: 7,910 segments\n"
     ]
    }
   ],
   "source": [
    "# Create specialized features for cold start (anonymous users)\n",
    "print(\"ğŸ†• Generating Cold Start Feature Matrix (Chunked)...\")\n",
    "\n",
    "chunk_size = 1_000_000\n",
    "n_rows = len(events_merged)\n",
    "cold_start_list = []\n",
    "\n",
    "for i in range(0, n_rows, chunk_size):\n",
    "    chunk = events_merged.iloc[i:i+chunk_size].copy()\n",
    "\n",
    "    # Extract anonymous user signals from events\n",
    "    anon = chunk.groupby(['category', 'region', 'Age', 'gender', 'source']).agg({\n",
    "        'user_pseudo_id': 'nunique',\n",
    "        'Item_revenue': 'mean',\n",
    "        'event_name': lambda x: (x == 'purchase').sum(),\n",
    "        'session_id': 'nunique'\n",
    "    }).reset_index()\n",
    "    # Calculate conversion rates by segment\n",
    "    anon['segment_conversion_rate'] = (anon['event_name'] / anon['session_id']).fillna(0)\n",
    "    anon['avg_revenue_per_user'] = anon['Item_revenue'].fillna(0)\n",
    "    anon['user_count'] = anon['user_pseudo_id']\n",
    "    # Create lookup table for cold start predictions\n",
    "    anon['segment_key'] = (\n",
    "        anon['category'].astype(str) + '_' +\n",
    "        anon['region'].astype(str) + '_' +\n",
    "        anon['Age'].astype(str) + '_' +\n",
    "        anon['gender'].astype(str) + '_' +\n",
    "        anon['source'].astype(str)\n",
    "    )\n",
    "    cold_start_list.append(anon)\n",
    "    del chunk, anon\n",
    "\n",
    "anonymous_features = pd.concat(cold_start_list)\n",
    "cold_start_lookup = anonymous_features.groupby('segment_key').agg({\n",
    "    'segment_conversion_rate': 'mean',\n",
    "    'avg_revenue_per_user': 'mean',\n",
    "    'user_count': 'sum'\n",
    "}).reset_index()\n",
    "# Keep only meaningful segments (min 10 users)\n",
    "cold_start_lookup = cold_start_lookup[cold_start_lookup['user_count'] >= 10].copy()\n",
    "\n",
    "\n",
    "print(f\"âœ… Generated cold start matrix: {len(cold_start_lookup):,} segments\")\n",
    "print(f\"ğŸ“Š Coverage: {cold_start_lookup['user_count'].sum():,} users in segments\")\n",
    "print(f\"ğŸ¯ Top converting segments:\")\n",
    "top_segments = cold_start_lookup.nlargest(3, 'segment_conversion_rate')[\n",
    "    ['segment_key', 'segment_conversion_rate', 'user_count']\n",
    "]\n",
    "print(top_segments.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699564f6",
   "metadata": {},
   "source": [
    "11: Feature Validation & Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef7b3ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Feature Validation & Quality Assessment...\n",
      "ğŸ“Š Master Features Quality Report:\n",
      "  â€¢ Total users: 744,675\n",
      "  â€¢ Total features: 34\n",
      "  â€¢ Memory usage: 808.6 MB\n",
      "\n",
      "ğŸ“‹ Feature Completeness Summary:\n",
      "  â€¢ RFM Features: 4/4 features available\n",
      "  â€¢ Geographic Features: 3/3 features available\n",
      "  â€¢ Journey Features: 3/3 features available\n",
      "  â€¢ Product Features: 3/3 features available\n",
      "\n",
      "ğŸ¯ User Segmentation Quality:\n",
      "Segment Performance Metrics:\n",
      "  â€¢ At_Risk: 53 users | Avg Sessions: 8.7 | Avg Revenue: $5131.49 | Conversion: 1.102\n",
      "  â€¢ Champions: 381,054 users | Avg Sessions: 1.3 | Avg Revenue: $4.78 | Conversion: 0.023\n",
      "  â€¢ Loyal_Customers: 1 users | Avg Sessions: 32.0 | Avg Revenue: $53653.14 | Conversion: 1.219\n",
      "  â€¢ New_Customers: 5 users | Avg Sessions: 975.2 | Avg Revenue: $0.00 | Conversion: 0.000\n",
      "  â€¢ Potential_Loyalists: 363,562 users | Avg Sessions: 1.3 | Avg Revenue: $4.16 | Conversion: 0.019\n",
      "\n",
      "ğŸ†• Cold Start Matrix Quality:\n",
      "  â€¢ Total segments: 7,910\n",
      "  â€¢ Users covered: 2,222,828\n",
      "  â€¢ Avg conversion rate: 0.0321\n",
      "\n",
      "ğŸ† Top Converting Segments (for demo):\n",
      "  â€¢ desktop_Massachusetts_35-44_female_google: 1.5810 conversion | 11 users\n",
      "  â€¢ mobile_Massachusetts_18-24_female_google: 1.2238 conversion | 10 users\n",
      "  â€¢ mobile_Illinois_55-64_male_(direct): 1.1238 conversion | 11 users\n",
      "\n",
      "âœ… VALIDATION COMPLETE - All features ready for ML pipeline!\n",
      "ğŸ’¾ Current memory usage: 8.5GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8.494831085205078"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 11: Feature Validation & Quality Check (STREAMLINED)\n",
    "print(\"ğŸ” Feature Validation & Quality Assessment...\")\n",
    "\n",
    "# Quick data quality metrics\n",
    "print(f\"ğŸ“Š Master Features Quality Report:\")\n",
    "print(f\"  â€¢ Total users: {len(master_features):,}\")\n",
    "print(f\"  â€¢ Total features: {master_features.shape[1]}\")\n",
    "print(f\"  â€¢ Memory usage: {master_features.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "# Feature completeness validation\n",
    "print(f\"\\nğŸ“‹ Feature Completeness Summary:\")\n",
    "feature_categories = {\n",
    "    'RFM Features': ['recency_days', 'frequency_score', 'monetary_score', 'conversion_rate'],\n",
    "    'Geographic Features': ['primary_region', 'primary_city', 'dominant_device'],\n",
    "    'Journey Features': ['preferred_page_type', 'page_diversity', 'journey_complexity'],\n",
    "    'Product Features': ['preferred_category', 'preferred_brand', 'price_sensitivity']\n",
    "}\n",
    "\n",
    "for category, features in feature_categories.items():\n",
    "    available_features = [f for f in features if f in master_features.columns]\n",
    "    print(f\"  â€¢ {category}: {len(available_features)}/{len(features)} features available\")\n",
    "\n",
    "# Segment quality validation\n",
    "print(f\"\\nğŸ¯ User Segmentation Quality:\")\n",
    "segment_analysis = master_features.groupby('segment_label').agg({\n",
    "    'total_sessions': 'mean',\n",
    "    'total_revenue': 'mean',\n",
    "    'conversion_rate': 'mean'\n",
    "}).round(3)\n",
    "\n",
    "print(\"Segment Performance Metrics:\")\n",
    "for segment, data in segment_analysis.iterrows():\n",
    "    user_count = (master_features['segment_label'] == segment).sum()\n",
    "    print(f\"  â€¢ {segment}: {user_count:,} users | \"\n",
    "          f\"Avg Sessions: {data['total_sessions']:.1f} | \"\n",
    "          f\"Avg Revenue: ${data['total_revenue']:.2f} | \"\n",
    "          f\"Conversion: {data['conversion_rate']:.3f}\")\n",
    "\n",
    "# Cold start coverage validation  \n",
    "print(f\"\\nğŸ†• Cold Start Matrix Quality:\")\n",
    "print(f\"  â€¢ Total segments: {len(cold_start_lookup):,}\")\n",
    "print(f\"  â€¢ Users covered: {cold_start_lookup['user_count'].sum():,}\")\n",
    "print(f\"  â€¢ Avg conversion rate: {cold_start_lookup['segment_conversion_rate'].mean():.4f}\")\n",
    "\n",
    "# Top performing segments for presentation\n",
    "print(f\"\\nğŸ† Top Converting Segments (for demo):\")\n",
    "top_segments = cold_start_lookup.nlargest(3, 'segment_conversion_rate')[\n",
    "    ['segment_key', 'segment_conversion_rate', 'user_count']\n",
    "]\n",
    "for _, row in top_segments.iterrows():\n",
    "    print(f\"  â€¢ {row['segment_key']}: {row['segment_conversion_rate']:.4f} conversion | {row['user_count']} users\")\n",
    "\n",
    "print(f\"\\nâœ… VALIDATION COMPLETE - All features ready for ML pipeline!\")\n",
    "check_memory()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a89447",
   "metadata": {},
   "source": [
    "12: Save Optimized Feature Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8dd1572b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Saving Production-Ready Feature Sets...\n",
      "ğŸ’¾ Saving master feature matrix...\n",
      "âœ… Master features: (744675, 34) | 17.2 MB\n",
      "ğŸ’¾ Saving session features...\n",
      "âœ… Session features: (1004683, 9) | 19.1 MB\n",
      "ğŸ’¾ Saving cold start lookup...\n",
      "âœ… Cold start lookup: (7910, 4) | 0.2 MB\n",
      "ğŸ’¾ Saving user summary...\n",
      "âœ… User summary: (744675, 8) | 10.2 MB\n",
      "ğŸ’¾ Creating segment profiles...\n",
      "âœ… Segment profiles saved for business analysis\n",
      "ğŸ’¾ Creating feature documentation...\n",
      "\n",
      "ğŸ“ FEATURE ENGINEERING COMPLETE!\n",
      "============================================================\n",
      "ğŸ“Š Files Created:\n",
      "  â€¢ cold_start_lookup.parquet: 0.2 MB\n",
      "  â€¢ feature_dictionary.json: 0.0 MB\n",
      "  â€¢ master_features.parquet: 17.2 MB\n",
      "  â€¢ segment_profiles.csv: 0.0 MB\n",
      "  â€¢ session_features.parquet: 19.1 MB\n",
      "  â€¢ user_summary.parquet: 10.2 MB\n",
      "\n",
      "ğŸ¯ PRODUCTION SUMMARY:\n",
      "  âœ… Total users processed: 744,675\n",
      "  âœ… Features generated: 34\n",
      "  âœ… User segments: 5\n",
      "  âœ… Cold start segments: 7,910\n",
      "  âœ… Total storage: 46.6 MB\n",
      "  âœ… Ready for Notebook 03: EDA & Segmentation\n",
      "ğŸ’¾ Current memory usage: 9.1GB\n",
      "\n",
      "ğŸ† NOTEBOOK 02 COMPLETE - FEATURE ENGINEERING SUCCESS!\n",
      "ğŸš€ Ready for recommendation engine and Streamlit prototype!\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Save Optimized Feature Sets (PRODUCTION-READY)\n",
    "print(\"ğŸ’¾ Saving Production-Ready Feature Sets...\")\n",
    "\n",
    "# Create optimized feature directory\n",
    "FEATURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1. Master feature matrix (complete user profiles)\n",
    "print(\"ğŸ’¾ Saving master feature matrix...\")\n",
    "master_features_optimized = master_features.copy()\n",
    "\n",
    "# Optimize data types for storage efficiency\n",
    "string_cols = master_features_optimized.select_dtypes(include=['object']).columns\n",
    "for col in string_cols:\n",
    "    master_features_optimized[col] = master_features_optimized[col].astype('category')\n",
    "\n",
    "# Save with optimal compression\n",
    "master_features_optimized.to_parquet(\n",
    "    FEATURES_DIR / \"master_features.parquet\", \n",
    "    compression='snappy',\n",
    "    index=True\n",
    ")\n",
    "print(f\"âœ… Master features: {master_features_optimized.shape} | \"\n",
    "      f\"{(FEATURES_DIR / 'master_features.parquet').stat().st_size / 1024**2:.1f} MB\")\n",
    "\n",
    "# 2. Session-level features (real-time processing)\n",
    "print(\"ğŸ’¾ Saving session features...\")\n",
    "session_features_final = session_features.select_dtypes(include=[np.number, 'bool']).copy()\n",
    "session_features_final.to_parquet(\n",
    "    FEATURES_DIR / \"session_features.parquet\", \n",
    "    compression='snappy'\n",
    ")\n",
    "print(f\"âœ… Session features: {session_features_final.shape} | \"\n",
    "      f\"{(FEATURES_DIR / 'session_features.parquet').stat().st_size / 1024**2:.1f} MB\")\n",
    "\n",
    "# 3. Cold start lookup (anonymous user engine)\n",
    "print(\"ğŸ’¾ Saving cold start lookup...\")\n",
    "cold_start_lookup.to_parquet(\n",
    "    FEATURES_DIR / \"cold_start_lookup.parquet\", \n",
    "    compression='snappy'\n",
    ")\n",
    "print(f\"âœ… Cold start lookup: {cold_start_lookup.shape} | \"\n",
    "      f\"{(FEATURES_DIR / 'cold_start_lookup.parquet').stat().st_size / 1024**2:.1f} MB\")\n",
    "\n",
    "# 4. User summary (lightweight API dataset)\n",
    "print(\"ğŸ’¾ Saving user summary...\")\n",
    "api_features = ['total_sessions', 'total_revenue', 'segment_label', \n",
    "                'dominant_device', 'primary_region', 'conversion_rate',\n",
    "                'preferred_category', 'price_sensitivity']\n",
    "available_api_features = [f for f in api_features if f in master_features.columns]\n",
    "\n",
    "user_summary = master_features[available_api_features].copy()\n",
    "user_summary.to_parquet(\n",
    "    FEATURES_DIR / \"user_summary.parquet\", \n",
    "    compression='snappy'\n",
    ")\n",
    "print(f\"âœ… User summary: {user_summary.shape} | \"\n",
    "      f\"{(FEATURES_DIR / 'user_summary.parquet').stat().st_size / 1024**2:.1f} MB\")\n",
    "\n",
    "# 5. Segment profiles (business intelligence)\n",
    "print(\"ğŸ’¾ Creating segment profiles...\")\n",
    "segment_profiles = master_features.groupby('segment_label').agg({\n",
    "    'total_sessions': ['mean', 'std'],\n",
    "    'total_revenue': ['mean', 'std'],\n",
    "    'conversion_rate': ['mean', 'std'],\n",
    "    'recency_days': 'mean',\n",
    "    'primary_region': lambda x: x.mode().iloc[0] if not x.mode().empty else 'unknown',\n",
    "    'dominant_device': lambda x: x.mode().iloc[0] if not x.mode().empty else 'unknown'\n",
    "}).round(3)\n",
    "\n",
    "# Flatten column names\n",
    "segment_profiles.columns = ['_'.join(col).strip() for col in segment_profiles.columns]\n",
    "segment_profiles.to_csv(FEATURES_DIR / \"segment_profiles.csv\")\n",
    "print(f\"âœ… Segment profiles saved for business analysis\")\n",
    "\n",
    "# 6. Feature dictionary (documentation)\n",
    "print(\"ğŸ’¾ Creating feature documentation...\")\n",
    "feature_dict = {\n",
    "    'total_features': master_features.shape[1],\n",
    "    'total_users': len(master_features),\n",
    "    'user_segments': master_features['segment_label'].value_counts().to_dict(),\n",
    "    'cold_start_segments': len(cold_start_lookup),\n",
    "    'data_coverage': {\n",
    "        'date_range': f\"{events_merged['eventDate'].min()} to {events_merged['eventDate'].max()}\",\n",
    "        'total_revenue': f\"${master_features['total_revenue'].sum():,.2f}\",\n",
    "        'conversion_rate': f\"{master_features['conversion_rate'].mean():.4f}\"\n",
    "    }\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(FEATURES_DIR / \"feature_dictionary.json\", 'w') as f:\n",
    "    json.dump(feature_dict, f, indent=2, default=str)\n",
    "\n",
    "# File inventory and summary\n",
    "print(f\"\\nğŸ“ FEATURE ENGINEERING COMPLETE!\")\n",
    "print(f\"=\" * 60)\n",
    "print(f\"ğŸ“Š Files Created:\")\n",
    "total_size = 0\n",
    "for file in FEATURES_DIR.glob(\"*\"):\n",
    "    size_mb = file.stat().st_size / (1024**2)\n",
    "    total_size += size_mb\n",
    "    print(f\"  â€¢ {file.name}: {size_mb:.1f} MB\")\n",
    "\n",
    "print(f\"\\nğŸ¯ PRODUCTION SUMMARY:\")\n",
    "print(f\"  âœ… Total users processed: {len(master_features):,}\")\n",
    "print(f\"  âœ… Features generated: {master_features.shape[1]}\")\n",
    "print(f\"  âœ… User segments: {len(master_features['segment_label'].unique())}\")\n",
    "print(f\"  âœ… Cold start segments: {len(cold_start_lookup):,}\")\n",
    "print(f\"  âœ… Total storage: {total_size:.1f} MB\")\n",
    "print(f\"  âœ… Ready for Notebook 03: EDA & Segmentation\")\n",
    "\n",
    "# Final memory cleanup\n",
    "del master_features_optimized, session_features_final, user_summary, segment_profiles\n",
    "gc.collect()\n",
    "check_memory()\n",
    "\n",
    "print(f\"\\nğŸ† NOTEBOOK 02 COMPLETE - FEATURE ENGINEERING SUCCESS!\")\n",
    "print(f\"ğŸš€ Ready for recommendation engine and Streamlit prototype!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eee54c8",
   "metadata": {},
   "source": [
    "13: Feature Dictionary & Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a48ea9c",
   "metadata": {},
   "source": [
    "## ğŸ—ƒï¸ Feature Dictionary & Documentation\n",
    "\n",
    "### **Master Features (master_features.parquet)**\n",
    "**User-level aggregated features for personalization engine**\n",
    "\n",
    "#### **RFM Features**\n",
    "- `recency_days`: Days since last user activity\n",
    "- `frequency_score`: Total number of sessions per user  \n",
    "- `monetary_score`: Total revenue attributed to user\n",
    "- `total_events`: Total events across all sessions\n",
    "- `conversion_rate`: Purchase events / total sessions\n",
    "- `avg_session_value`: Revenue per session average\n",
    "\n",
    "#### **Geographic & Demographic Features**\n",
    "- `primary_region`: Most frequent geographic region\n",
    "- `primary_city`: Most frequent city location\n",
    "- `dominant_device`: Most used device type (mobile/desktop/tablet)\n",
    "- `primary_source`: Most frequent traffic source\n",
    "- `primary_gender`: User gender (if available)\n",
    "- `primary_age`: User age group classification\n",
    "- `primary_income`: User income segment\n",
    "\n",
    "#### **Behavioral Features**\n",
    "- `preferred_page_type`: Most visited page category\n",
    "- `page_diversity`: Number of unique pages visited\n",
    "- `journey_complexity`: Unique event types in user journey\n",
    "- `device_diversity`: Number of different devices used\n",
    "- `source_diversity`: Number of different traffic sources\n",
    "\n",
    "#### **Product Affinity Features** \n",
    "- `preferred_category`: Most purchased product category\n",
    "- `preferred_brand`: Most purchased brand\n",
    "- `product_diversity`: Number of unique products purchased\n",
    "- `avg_item_price`: Average purchase price\n",
    "- `price_sensitivity`: Budget/mid/premium/luxury classification\n",
    "\n",
    "#### **Segmentation**\n",
    "- `user_segment`: Numeric cluster ID (0-4)\n",
    "- `segment_label`: Business-friendly segment name\n",
    "\n",
    "### **Session Features (session_features.parquet)**\n",
    "**Session-level features for real-time personalization**\n",
    "\n",
    "- `session_hour`: Hour of day session started\n",
    "- `is_weekend`: Boolean weekend indicator\n",
    "- `duration_minutes`: Session length in minutes\n",
    "- `events_per_minute`: Engagement intensity metric\n",
    "- `has_purchase`: Boolean purchase indicator\n",
    "- `session_value`: Revenue generated in session\n",
    "- `time_of_day`: Categorical time period\n",
    "\n",
    "### **Cold Start Lookup (cold_start_lookup.parquet)**\n",
    "**Anonymous user prediction matrix**\n",
    "\n",
    "- `segment_key`: Combined demographic+device+source identifier\n",
    "- `segment_conversion_rate`: Historical conversion rate for segment\n",
    "- `avg_revenue_per_user`: Expected revenue for segment\n",
    "- `user_count`: Number of historical users in segment\n",
    "\n",
    "**ğŸ¯ TOTAL FEATURES GENERATED: 30+ personalization features ready for ML pipeline**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11dce94",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aignition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
